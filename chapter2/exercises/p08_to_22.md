# 演習問題 8 to 19.

## 8

```
>>> names = nltk.corpus.names
>>> cfd = nltk.ConditionalFreqDist(
    (fileid, name[0])
    for fileid in names.fileids()
    for name in names.words(fileid))
>>> cfd.plot()
```

* H,Wで始まる名前は女性が多い
* それ以外は同等または男性の方が多い

## 9

```
>>> from nltk.book import *
*** Introductory Examples for the NLTK Book ***
Loading text1, ..., text9 and sent1, ..., sent9
Type the name of the text or sentence to view it.
Type: 'texts()' or 'sents()' to list the materials.
text1: Moby Dick by Herman Melville 1851
text2: Sense and Sensibility by Jane Austen 1811
...
```

* 語彙と語彙の豊富さのために重複の排除をしてみた
* これを行う関数が存在した…

```
>>> [(len(t), len(set(t))) for t in [text1, text2]]
Out[116]: [(260819, 19317), (141576, 6833)]
```

```
>>> text1.vocab()
Building vocabulary index...
Out[119]: <FreqDist with 19317 samples and 260819 outcomes>
>>> text2.vocab()
Building vocabulary index...
Out[120]: <FreqDist with 6833 samples and 141576 outcomes>
```

```
>>> t1_vocab = text1.vocab()
>>> type(t1_vocab)
Out[123]: nltk.probability.FreqDist
```

* ジャンル？
* よくわからない

## 10

* 9のテキストを使用
* vocabularyが頻度分布なので助かった
* 以下のとおり、3分の1を占めている

```
>>> from __future__ import division
>>> sum(n for w,n in t1_vocab.items()[:20]) / len(text1)
Out[129]: 0.3540807993282698
```

```
>>> t2_vocab = text2.vocab()
>>> sum(n for w,n in t2_vocab.items()[:20]) / len(text2)
Out[131]: 0.3566776854834153
```

## 11

* 法助動詞は、目次によるとP.201ページ
* 5.2 タグ付きコーパス
* 5.2.6 形容詞と動詞
* 上記より、should,mayなどとのこと
* 2.1.6 注釈付きのテキストコーパスには法助動詞が出てこない
* TODO：全部

## 12

```
>>> entries = nltk.corpus.cmudict.entries()
>>> cfd = nltk.ConditionalFreqDist(
    ('test', word)
    for word, l in entries)
>>> len(cfd['test'].items())
Out[23]: 123455
>>> len([w for w,n in cfd['test'].items() if n > 1])
Out[24]: 9241
```

* TODO:どの部分

## 13

```
>>> from nltk.corpus import wordnet as wn
>>> wn.all_synsets('n')
>>> len([v for v in wn.all_synsets('n')])
Out[30]: 82115
>>> len([v for v in wn.all_synsets('n') if len(v.hyponyms()) == 0])
Out[31]: 65422
```

## 14

* ソース参照
* doc commet(param type, return type)についてもやってみた

## 15

```
>>> from nltk.corpus import brown
>>> fdist = nltk.FreqDist(brown.words())
>>> more_three_words = [w for w,n in fdist.items() if n >= 3]
```

## 16

* TODO

## 17

```
>>> stopwords = nltk.corpus.stopwords.words('english')
>>> [w.lower() for w,n in nltk.FreqDist(nltk.corpus.reuters.words()).items() if not w.lower() in set(stopwords)][:50]
```

## 18

```
nltk.bigrams(17で求めた単語一覧)
```

## 19

## 20

## 21

## 22

* 半黒丸のものだけを抜粋している
